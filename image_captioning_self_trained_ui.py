# -*- coding: utf-8 -*-
"""image captioning self trained ui.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13wnap6UK6Sa5v9DER3O_kuwmuk9FIHVz
"""

# ========================= IMPORTS =========================
import torch
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from PIL import Image
import gradio as gr

# ========================= DEVICE =========================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Using device: {device}")

# ========================= LOAD MODEL =========================
MODEL_NAME = "nlpconnect/vit-gpt2-image-captioning"
SAVE_PATH = "/content/drive/MyDrive/fine_tuned_caption_model.pth"

# Load base model and then fine-tuned weights
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.load_state_dict(torch.load(SAVE_PATH, map_location=device))
model.to(device)
model.eval()

# Load feature extractor and tokenizer
feature_extractor = ViTImageProcessor.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ========================= CAPTION FUNCTION =========================
def generate_caption(image):
    if image.mode != "RGB":
        image = image.convert("RGB")

    pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values.to(device)
    with torch.no_grad():
        output_ids = model.generate(pixel_values, max_length=40, num_beams=4)
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return caption

# ========================= GRADIO INTERFACE =========================
iface = gr.Interface(
    fn=generate_caption,
    inputs=gr.Image(type="pil"),
    outputs=gr.Textbox(label="Generated Caption"),
    title="Mini COCO Image Captioning",
    description="Upload an image and get a caption generated by the fine-tuned model."
)

iface.launch()

!pip install -q huggingface_hub gradio

from huggingface_hub import notebook_login
notebook_login()

from huggingface_hub import whoami
whoami()

from huggingface_hub import create_repo
create_repo("image-caption-demo", repo_type="space", space_sdk="gradio")

from huggingface_hub import upload_file
import os

# =====================
# CONFIG
# =====================
REPO_ID = "Rushiparhad/image-caption-demo"   # üëà Your Hugging Face Space name
MODEL_PATH = "/content/drive/MyDrive/fine_tuned_caption_model.pth"  # üëà Path to your model (update if needed)
APP_PATH = "/content/app.py"  # üëà Path to your Gradio script (we‚Äôll create this below)

# =====================
# CHECK FILES EXIST
# =====================
assert os.path.exists(MODEL_PATH), "‚ùå Model file not found!"
print(f"‚úÖ Model found: {MODEL_PATH}")

# =====================
# UPLOAD MODEL
# =====================
upload_file(
    path_or_fileobj=MODEL_PATH,
    path_in_repo="fine_tuned_caption_model.pth",
    repo_id=REPO_ID,
    repo_type="space"
)
print("‚úÖ Model uploaded to Space")

# =====================
# CREATE GRADIO APP FILE
# =====================
app_code = """
import gradio as gr
import torch
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from PIL import Image

MODEL_PATH = "fine_tuned_caption_model.pth"
MODEL_NAME = "nlpconnect/vit-gpt2-image-captioning"

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load base model and components
feature_extractor = ViTImageProcessor.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)

# Load fine-tuned weights
state_dict = torch.load(MODEL_PATH, map_location=device)
model.load_state_dict(state_dict, strict=False)
model.to(device)
model.eval()

def caption_image(image):
    pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values.to(device)
    with torch.no_grad():
        output_ids = model.generate(pixel_values, max_length=40, num_beams=4)
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return caption

demo = gr.Interface(fn=caption_image, inputs=gr.Image(type="pil"), outputs="text", title="Image Caption Generator")

demo.launch()
"""

with open(APP_PATH, "w") as f:
    f.write(app_code)

print("‚úÖ app.py created")

# =====================
# UPLOAD GRADIO APP
# =====================
upload_file(
    path_or_fileobj=APP_PATH,
    path_in_repo="app.py",
    repo_id=REPO_ID,
    repo_type="space"
)
print("‚úÖ app.py uploaded successfully")

print(f"\nüéâ Your app is ready! Visit: https://huggingface.co/spaces/{REPO_ID}")

# ========================= IMPORTS =========================
!pip install evaluate nltk rouge_score pycocoevalcap -q

import os, json, torch
from PIL import Image
from tqdm import tqdm
import evaluate
import nltk

from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer

# ========================= NLTK SETUP =========================
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')

# ========================= DEVICE =========================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Using device: {device}")

# ========================= PATHS =========================
BASE_MODEL = "nlpconnect/vit-gpt2-image-captioning"
MODEL_PATH = "/content/drive/MyDrive/fine_tuned_caption_model.pth"
ANNOTATIONS_PATH = "/content/drive/MyDrive/mini_coco2014/captions.json"
IMAGES_DIR = "/content/drive/MyDrive/mini_coco2014/Images"

# ========================= LOAD MODEL =========================
model = VisionEncoderDecoderModel.from_pretrained(BASE_MODEL)
model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
model.to(device)
model.eval()

# ========================= LOAD TOKENIZER & PROCESSOR =========================
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
feature_extractor = ViTImageProcessor.from_pretrained(BASE_MODEL)

# ========================= LOAD DATA =========================
with open(ANNOTATIONS_PATH, "r") as f:
    data = json.load(f)

# Handle structure ‚Äî either {"annotations": [...]} or flat list
samples = data["annotations"] if isinstance(data, dict) and "annotations" in data else data
print(f"‚úÖ Loaded {len(samples)} samples")

# ========================= LOAD METRICS =========================
bleu_metric = evaluate.load("bleu")
meteor_metric = evaluate.load("meteor")
rouge_metric = evaluate.load("rouge")

try:
    cider_metric = evaluate.load("cider")
    use_cider = True
except:
    print("‚ö†Ô∏è CIDEr metric not found, skipping it.")
    use_cider = False

# ========================= GENERATE CAPTIONS =========================
predictions, references = [], []

for i, sample in enumerate(tqdm(samples[:300], desc="Evaluating")):  # evaluate up to 300 samples
    img_id = sample.get("image") or sample.get("file_name") or sample.get("image_id")
    caption = sample.get("caption") or sample.get("text")

    if img_id is None or caption is None:
        continue

    img_path = os.path.join(IMAGES_DIR, img_id)
    if not os.path.exists(img_path):
        continue

    try:
        image = Image.open(img_path).convert("RGB")
        pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values.to(device)

        with torch.no_grad():
            output_ids = model.generate(pixel_values, max_length=40, num_beams=4)
        generated_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        predictions.append(generated_caption)
        references.append([caption])

    except Exception as e:
        print(f"‚ö†Ô∏è Skipping {img_id} due to error: {e}")
        continue

print(f"‚úÖ Finished generating captions. Total valid pairs: {len(predictions)}")

# ========================= CLEAN INVALID ENTRIES =========================
clean_preds, clean_refs = [], []
for p, r in zip(predictions, references):
    if isinstance(p, str) and isinstance(r, list) and len(r) > 0:
        clean_preds.append(p)
        clean_refs.append(r)

print(f"‚úÖ Cleaned dataset: {len(clean_preds)} valid pairs (removed {len(predictions) - len(clean_preds)} invalid)")

# ========================= COMPUTE METRICS =========================
if len(clean_preds) > 0:
    bleu_score = bleu_metric.compute(predictions=clean_preds, references=clean_refs)
    meteor_score = meteor_metric.compute(predictions=clean_preds, references=clean_refs)
    rouge_score = rouge_metric.compute(predictions=clean_preds, references=clean_refs)
    cider_score = cider_metric.compute(predictions=clean_preds, references=clean_refs) if use_cider else {"score": "N/A"}

    # Handle ROUGE-L return type (float or object)
    rouge_l_value = (
        rouge_score["rougeL"]
        if isinstance(rouge_score["rougeL"], (float, int))
        else rouge_score["rougeL"].mid.fmeasure
    )

    print("\nüìä Evaluation Results:")
    print(f"BLEU Score   : {bleu_score['bleu']:.4f}")
    print(f"METEOR Score : {meteor_score['meteor']:.4f}")
    print(f"ROUGE-L Score: {rouge_l_value:.4f}")
    print(f"CIDEr Score  : {cider_score['score']}")
else:
    print("‚ö†Ô∏è No valid pairs to evaluate. Check dataset and captions.")

!pip install -q evaluate rouge_score

with open(ANNOTATIONS_PATH, "r") as f:
    data = json.load(f)
print(data.keys())      # To check what keys exist
print(type(data))

print(data["annotations"][0])